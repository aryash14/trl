{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashahvar\\AppData\\Local\\miniconda3\\envs\\rlhf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, set_seed\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fully working simple example to use trl with accelerate.\n",
    "This example fine-tunes a T5 model on the IMDB dataset using PPO\n",
    "(proximal policy optimization).\n",
    "in any of the following settings (with the same script):\n",
    "  - single CPU or single GPU\n",
    "  - multi GPUS (using PyTorch distributed mode)\n",
    "  - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)\n",
    "  - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "To run it in each of these various modes, first initialize the accelerate\n",
    "configuration with `accelerate config` then run the script with\n",
    "`accelerate launch ppo-sentiment-t5-small.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first define the configuration of the experiment, defining the model, the dataset,\n",
    "# the training parameters, and the PPO parameters.\n",
    "# Check the default arguments in the `PPOConfig` class for more details.\n",
    "config = PPOConfig(model_name=\"lvwerra/t5-imdb\", learning_rate=5e-5, batch_size=32)\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.forward_batch_size}\n",
    "sent_kwargs['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_imdb_dataset(tokenizer, input_min_text_length=2, input_max_text_length=8):\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(\"imdb\", split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()] + [tokenizer.eos_token_id]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def collater(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed before initializing value head for deterministic eval\n",
    "# set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (C:/Users/ashahvar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\ashahvar\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-b3cf6ed352eaef2a.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\ashahvar\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-278994d60a2a151b.arrow\n"
     ]
    }
   ],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_imdb_dataset(tokenizer)\n",
    "\n",
    "generation_kwargs = {\"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"eos_token_id\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Forward batch size > 1 is not well supported yet for encoder-decoder models and when using `tokenizer.padding_side='left'`. This can lead to unexpected behaviour. therefore, we recommend using forward_batch_size=1.\n"
     ]
    }
   ],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collater)\n",
    "\n",
    "# We then build the sentiment analysis pipeline, passing the model name and the\n",
    "# sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "# to the same device as the PPOTrainer.\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \"lvwerra/distilbert-imdb\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 2.557040214538574}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this movie was really bad!!'\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_min_length = 16\n",
    "output_max_length = 32\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "gen_len = output_length_sampler()\n",
    "-gen_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [02:05, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "output_min_length = 16\n",
    "output_max_length = 32\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # print(len(batch['label']))\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze())\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    # print(pipe_outputs)\n",
    "    rewards = [ torch.tensor(output[\"score\"] * -1).to(device) if output[\"label\"] == \"NEGATIVE\" else torch.tensor(output[\"score\"]).to(device) for output in pipe_outputs]\n",
    "    # print(len(rewards), len(batch[\"response\"]), batch[\"response\"])\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this send-up&lt;/s&gt;</td>\n",
       "      <td>prototypical, non-grid film that allows you to...</td>\n",
       "      <td>m of their views in take-home from a larger sc...</td>\n",
       "      <td>1.015245</td>\n",
       "      <td>0.900952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Rock Collier sounds like you need a daug...</td>\n",
       "      <td>&lt;pad&gt; very interesting and wonderful, rather t...</td>\n",
       "      <td>0.431099</td>\n",
       "      <td>2.491739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm writing this because I&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; this one shines with each proverbial typ...</td>\n",
       "      <td>&lt;pad&gt; the sequel is packed full of great music...</td>\n",
       "      <td>2.426240</td>\n",
       "      <td>2.544981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Shadrach\" was&lt;/s&gt;</td>\n",
       "      <td>. His expertly-manned ones expressed credit to...</td>\n",
       "      <td>couple of years and a half!) It was operating ...</td>\n",
       "      <td>1.429433</td>\n",
       "      <td>0.489971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some guys think that sniper&lt;/s&gt;</td>\n",
       "      <td>war was about dumb and tongue downtrice, but h...</td>\n",
       "      <td>enjoys a srt like shooter, but filmed by his f...</td>\n",
       "      <td>0.983530</td>\n",
       "      <td>0.122268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anthony Mann's westerns&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; appearance by the local preacher. He als...</td>\n",
       "      <td>&lt;pad&gt; the way that are highlighted by this rev...</td>\n",
       "      <td>2.082433</td>\n",
       "      <td>2.348679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I recently saw this film and enjoyed&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; when I first saw Kelli Sisley and Advin ...</td>\n",
       "      <td>&lt;pad&gt; liked the fulfilment of their disappoint...</td>\n",
       "      <td>1.543884</td>\n",
       "      <td>-0.035536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Teresa Pavline&lt;/s&gt;</td>\n",
       "      <td>r /&gt;&lt;unk&gt;br /&gt;Our Saga goes crazy and we are in a</td>\n",
       "      <td>in return for hard work invited from free town...</td>\n",
       "      <td>0.601710</td>\n",
       "      <td>1.100774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>My children just happened to stop&lt;/s&gt;</td>\n",
       "      <td>They came back on their outer relationship wit...</td>\n",
       "      <td>I have one problem with being bad with reality...</td>\n",
       "      <td>0.369392</td>\n",
       "      <td>1.237059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Steven Seagal movies have&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; of a seemingly horrible nature. Key to t...</td>\n",
       "      <td>&lt;pad&gt; play some kind of I wouldn't think to be...</td>\n",
       "      <td>0.814929</td>\n",
       "      <td>1.930972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I voted excellent for how&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; I honestly love movies. I think his film...</td>\n",
       "      <td>&lt;pad&gt;s because, in the end, the plot is too sp...</td>\n",
       "      <td>2.553799</td>\n",
       "      <td>0.981989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>please save your&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; their adorable birthday flick through th...</td>\n",
       "      <td>&lt;pad&gt; green script.,CeltoTranscripts gives you...</td>\n",
       "      <td>1.603837</td>\n",
       "      <td>0.275415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lillian Hellman's&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt;'s string of characters is far from chill...</td>\n",
       "      <td>&lt;pad&gt; of Kandidata Liess, sentenced to life, h...</td>\n",
       "      <td>1.081734</td>\n",
       "      <td>0.375646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I borrowed this movie despite&lt;/s&gt;</td>\n",
       "      <td>this movie because it did not turn out to be a...</td>\n",
       "      <td>and off The Angeles/USA. It gave such highly a...</td>\n",
       "      <td>0.648889</td>\n",
       "      <td>2.239319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I will be short...&lt;/s&gt;</td>\n",
       "      <td>float in price. I just enjoyed it alot better ...</td>\n",
       "      <td>If anyone is interested in this film, this was...</td>\n",
       "      <td>1.824559</td>\n",
       "      <td>0.219114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>After some of the negative reviews&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt;(2), the stories were exactly what the st...</td>\n",
       "      <td>&lt;pad&gt; I have requested the review who has take...</td>\n",
       "      <td>1.516139</td>\n",
       "      <td>1.111948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       query  \\\n",
       "0                        In this send-up</s>   \n",
       "1                             This movie</s>   \n",
       "2             I'm writing this because I</s>   \n",
       "3                         \"Shadrach\" was</s>   \n",
       "4            Some guys think that sniper</s>   \n",
       "5                Anthony Mann's westerns</s>   \n",
       "6   I recently saw this film and enjoyed</s>   \n",
       "7                         Teresa Pavline</s>   \n",
       "8      My children just happened to stop</s>   \n",
       "9              Steven Seagal movies have</s>   \n",
       "10             I voted excellent for how</s>   \n",
       "11                      please save your</s>   \n",
       "12                     Lillian Hellman's</s>   \n",
       "13         I borrowed this movie despite</s>   \n",
       "14                    I will be short...</s>   \n",
       "15    After some of the negative reviews</s>   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0   prototypical, non-grid film that allows you to...   \n",
       "1   <pad> Rock Collier sounds like you need a daug...   \n",
       "2   <pad> this one shines with each proverbial typ...   \n",
       "3   . His expertly-manned ones expressed credit to...   \n",
       "4   war was about dumb and tongue downtrice, but h...   \n",
       "5   <pad> appearance by the local preacher. He als...   \n",
       "6   <pad> when I first saw Kelli Sisley and Advin ...   \n",
       "7   r /><unk>br />Our Saga goes crazy and we are in a   \n",
       "8   They came back on their outer relationship wit...   \n",
       "9   <pad> of a seemingly horrible nature. Key to t...   \n",
       "10  <pad> I honestly love movies. I think his film...   \n",
       "11  <pad> their adorable birthday flick through th...   \n",
       "12  <pad>'s string of characters is far from chill...   \n",
       "13  this movie because it did not turn out to be a...   \n",
       "14  float in price. I just enjoyed it alot better ...   \n",
       "15  <pad>(2), the stories were exactly what the st...   \n",
       "\n",
       "                                     response (after)  rewards (before)  \\\n",
       "0   m of their views in take-home from a larger sc...          1.015245   \n",
       "1   <pad> very interesting and wonderful, rather t...          0.431099   \n",
       "2   <pad> the sequel is packed full of great music...          2.426240   \n",
       "3   couple of years and a half!) It was operating ...          1.429433   \n",
       "4   enjoys a srt like shooter, but filmed by his f...          0.983530   \n",
       "5   <pad> the way that are highlighted by this rev...          2.082433   \n",
       "6   <pad> liked the fulfilment of their disappoint...          1.543884   \n",
       "7   in return for hard work invited from free town...          0.601710   \n",
       "8   I have one problem with being bad with reality...          0.369392   \n",
       "9   <pad> play some kind of I wouldn't think to be...          0.814929   \n",
       "10  <pad>s because, in the end, the plot is too sp...          2.553799   \n",
       "11  <pad> green script.,CeltoTranscripts gives you...          1.603837   \n",
       "12  <pad> of Kandidata Liess, sentenced to life, h...          1.081734   \n",
       "13  and off The Angeles/USA. It gave such highly a...          0.648889   \n",
       "14  If anyone is interested in this film, this was...          1.824559   \n",
       "15  <pad> I have requested the review who has take...          1.516139   \n",
       "\n",
       "    rewards (after)  \n",
       "0          0.900952  \n",
       "1          2.491739  \n",
       "2          2.544981  \n",
       "3          0.489971  \n",
       "4          0.122268  \n",
       "5          2.348679  \n",
       "6         -0.035536  \n",
       "7          1.100774  \n",
       "8          1.237059  \n",
       "9          1.930972  \n",
       "10         0.981989  \n",
       "11         0.275415  \n",
       "12         0.375646  \n",
       "13         2.239319  \n",
       "14         0.219114  \n",
       "15         1.111948  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data['query'] = df_batch['query'].tolist()\n",
    "query_tensors = df_batch['input_ids'].tolist()\n",
    "\n",
    "# print(game_data['query'])\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "                                    **generation_kwargs).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "                                 **generation_kwargs).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data['response (before)'] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data['response (after)'] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q,r in zip(game_data['query'], game_data['response (before)'])]\n",
    "game_data['rewards (before)'] = [output[\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q,r in zip(game_data['query'], game_data['response (after)'])]\n",
    "game_data['rewards (after)'] = [output[\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "import pandas as pd\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc79c4d8bd33bb9cdbd47d630b7971ab9897b56f1c458ef4d9ed0943aff5a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
